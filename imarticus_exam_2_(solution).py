# -*- coding: utf-8 -*-
"""Imarticus Exam -2 (Solution).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c8x0lLpfKIqfcJy8ZA3cXL5hOGnaWpc9
"""

# Import drive

from google.colab import drive
drive.mount('/content/drive')

# importing the dataset

import pandas as pd

df=pd.read_csv('/content/drive/MyDrive/Imarticus Datasets/Decision Tree + Random Forest + Ensemble Techniques/admission.csv')

"""Q.1 Perform Exploratory Data Analysis (EDA)

a) Visualize the 10 random rows of the data set
"""

df.sample(10)

df=df.drop(['Serial No.'],axis=1)

"""b) Generate the description for numeric variables"""

df.describe()

# dropping the dummies

df.drop_duplicates(inplace=True)

df.info()

# cheaking for missing values

df.isna().sum().sort_values(ascending=False)

"""c) Check the shape of the data set"""

df.shape

df.info()

"""d) Generate the correlation matrix"""

df.corr()

#Visualize the  correlation for the given datasets 
df.corr()['Chance of Admit ']

"""e) Generate a correlogram"""

#Distribution of dataset columns
fig=df.hist(figsize=(10,10),color='lightblue')
plt.show()

import seaborn as sns

#Pairplot for the datasets
sns.pairplot(df)

# heatmap
sns.heatmap(df.corr(),annot=True,linewidths=1.0)
plt.show()

"""Q.2 Find out the minimum and maximum values for GRE
score
"""

print('Minimum values for GRE score :',df['GRE Score'].min())
print('Maximum values for GRE score :',df['GRE Score'].max())

"""Q.3 Find out the percentage of universities for each
university rating
"""

rating=df['University Rating'].unique()

rating.sort()
rating

c=[]
v=list(df['University Rating'].values)
for i in rating:
  c.append(v.count(i))

print(c)

# result

print("University Rating Percentage:")
for i in range(0,5):
  print('University Rating',i+1,' :',c[i]/5,'%')

"""Q.4
Convert the target variable “Chance of Admit” to
categorical having values 0 and 1:

1. Students having the “Chance of Admit” value > 0.80, are assigned
value 1, and

2. Students having the “Chance of Admit” value < 0.80, are assigned
value 0

Where 0: Low chance of Admission and 1: High chance of
admission
"""

v=list(df['Chance of Admit '].values)
for i in v:
  if i>0.80:
    df['Chance of Admit '] = df['Chance of Admit '].replace({i : 1})
  else:
    df['Chance of Admit '] = df['Chance of Admit '].replace({i : 0})

# converting Chance of Admit in float64 to int64

df['Chance of Admit ']=df['Chance of Admit '].astype(int)

print(df['Chance of Admit '])

"""Q.5 Build a Decision Tree classifier, to predict whether a
student has a low or high chance of admission to a chosen
university. Perform Hyperparameter Tuning to improve
the accuracy of the model.

Build a Decision Tree classifier
"""

# train and test

x = df.drop(['Chance of Admit '],axis=1)

y= df['Chance of Admit ']

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2,random_state=0)

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()

model.fit(xtrain,ytrain)

ypred = model.predict(xtest)

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

acc1=accuracy_score(ytest,ypred)
print('Decision Tree Classifier accuracy score :',acc1)

confusion_matrix(ytest,ypred)

print(classification_report(ytest,ypred))

"""Hyperparameter Tuning to improve the accuracy of the model"""

# Create an scaler object
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

# Create a pca object
from sklearn import decomposition

pca = decomposition.PCA()

# Create a pipeline of three steps. First, standardize the data.
# Second, tranform the data with PCA.
# Third, train a Decision Tree Classifier on the data.
from sklearn.pipeline import Pipeline

pipe = Pipeline(steps=[('sc', sc), 
                       ('pca', pca),
                       ('decisiontree', model)])

# Create Parameter Space
# Create a list of a sequence of integers from 1 to 30 (the number of features in X + 1)
n_components = list(range(1,x.shape[1]+1,1))

# Create lists of parameter for Decision Tree Classifier
criterion = ['gini', 'entropy']

max_depth = [2,4,6,8,12]

# Create a dictionary of all the parameter options 
# Note has you can access the parameters of steps of a pipeline by using '__’
parameters = dict(pca__n_components=n_components,
                  decisiontree__criterion=criterion,
                  decisiontree__max_depth=max_depth)

# Conduct Parameter Optmization With Pipeline
# Create a grid search object
from sklearn.model_selection import GridSearchCV, cross_val_score

clf = GridSearchCV(pipe, parameters)

# Fit the grid search
clf.fit(x, y)

# View The Best Parameters
print('Best Criterion:', clf.best_estimator_.get_params()['decisiontree__criterion'])

print('Best max_depth:', clf.best_estimator_.get_params()['decisiontree__max_depth'])

print('Best Number Of Components:', clf.best_estimator_.get_params()['pca__n_components'])

print(clf.best_estimator_.get_params()['decisiontree'])

# Use Cross Validation To Evaluate Model
CV_Result = cross_val_score(clf, x, y, cv=4, n_jobs=-1)

print(CV_Result)

acc2=CV_Result.mean()
print('Decision Tree Classifier Hyperparameter Tuning accuracy score :',acc2)

print(CV_Result.std())

"""Q.6 Build a Random Forest classifier, to predict whether a
student has a low or high chance of admission to a chosen
university.

Build a Random Forest classifier
"""

# Build a random forest classifier
# Creating a random forest classifier
from sklearn.ensemble import RandomForestClassifier

clf_rf = RandomForestClassifier(n_estimators = 200, criterion = 'entropy', 
                                min_samples_split = 10, min_samples_leaf = 9, max_features = "auto",
                                random_state = 500, max_depth = 12)

# Training the model 
clf_fit = clf_rf.fit(xtrain, ytrain) 

# Predicting the quality
y_pred = clf_fit.predict(xtest)

# Checking the accuracy of the model
acc3=accuracy_score(ytest, y_pred)
print('Random Forest classifier accuracy score :',acc3)

# Predicting the probabilities of wine being of high quality
y_proba = clf_fit.predict_proba(xtest)

# Visualizing the confusion matrix
print(confusion_matrix(ytest, y_pred))

print(classification_report(ytest, y_pred))

# Visualizing the ROC-AUC curve 

# We take the predicted values of class 1
y_predicted = y_proba[:,1]

# We check to see if the right values have been considered from the predicted values
print(y_predicted)

# Using roc_curve() to generate fpr & tpr values
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(ytest,y_predicted)

# Passing the fpr & tpr values to auc() to calculate the area under curve
from sklearn.metrics import auc
roc_auc = auc(fpr,tpr)
roc1=roc_auc
print("Area under the curve for first model",roc_auc)

# Plotting the ROC curve
import matplotlib.pyplot as plt

plt.figure()
plt.plot(fpr, tpr, color = 'orange', lw = 2, label = 'ROC curve (area under curve =%0.2f)'%roc_auc)

plt.plot([0,1],[0,1], color = 'darkgrey',lw = 2,linestyle='--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.xlabel('False Positive Rate (1-Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('ROC Curve for first model')
plt.legend(loc = "upper left")
plt.show()

# Again creating a random forest classifier
clf_rf2 = RandomForestClassifier(n_estimators = 100, criterion = 'gini', 
                                min_samples_split = 2, min_samples_leaf = 5, max_features = "auto",
                                random_state = 100, max_depth = 3)

# Training the model 
clf_fit_2 = clf_rf2.fit(xtrain, ytrain) 

# Predicting the quality 
y_pred_2 = clf_fit_2.predict(xtest)

# Checking the accuracy of the model
from sklearn.metrics import accuracy_score
accuracy_score(ytest, y_pred_2)

# Predicting the probabilities of wine being of high quality using the second model
y_proba_2 = clf_fit.predict_proba(xtest)

# Visualizing the ROC-AUC curve 

# We take the predicted values of class 1
y_predicted_2 = y_proba_2[:,1]

# Using roc_curve() to generate fpr & tpr values
fpr, tpr, thresholds = roc_curve(ytest, y_predicted_2)

# Passing the fpr & tpr values to auc() to calculate the area under curve
from sklearn.metrics import auc
roc_auc = auc(fpr,tpr)
roc2=roc_auc
print("Area under the curve for the second model",roc_auc)

# Plotting the ROC curve
import matplotlib.pyplot as plt

plt.figure()
plt.plot(fpr, tpr, color = 'orange', lw = 2, label = 'ROC curve (area under curve =%0.2f)'%roc_auc)

plt.plot([0,1],[0,1], color = 'darkgrey',lw = 2,linestyle='--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.0])
plt.xlabel('False Positive Rate (1-Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('ROC Curve for second model')
plt.legend(loc = "upper left")
plt.show()

"""Q.7 Also use Ensemble Modelling techniques, to predict
whether a student has a low or high chance of admission
to a chosen university.

Bagging technique
"""

from sklearn.ensemble import BaggingClassifier
from sklearn import tree

# build the model
meta_estimator = BaggingClassifier(tree.DecisionTreeClassifier(random_state=10))

# fit the model
meta_estimator.fit(xtrain, ytrain)

from sklearn import metrics

bagging_score = metrics.roc_auc_score(ytest, y_pred)

print('Bagging score :',bagging_score)

"""AdaBoost technique """

from sklearn.ensemble import AdaBoostClassifier

# build the model
adaboost = AdaBoostClassifier(random_state=10)
# fit the model
adaboost.fit(xtrain, ytrain)

# predict the values
y_pred_adaboost  = adaboost.predict(xtest)

adaboost_score = metrics.roc_auc_score(ytest, y_pred_adaboost)

print('Adaboost score :',bagging_score)

result = pd.DataFrame()
result['Model'] = ['Bagging','AdaBoost']
result['Accuracy score'] = [bagging_score,adaboost_score]

result

"""Q.8 Compare all of the models and justify your choice about
the optimum model.
"""

r = pd.DataFrame()
r['Model'] = ['Decision Tree classifier','DecisionTreeClassifier Hyperparameter Tuning ','Random Forest classifier','Bagging','AdaBoost']
r['Accuracy Score'] = [acc1,acc2,acc3,bagging_score,adaboost_score]

r

"""Random forest gives us the best accuracy score so it is the best model here. """